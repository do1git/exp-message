# 패치노트 - 실시간 통신 최적화 (기획)

## Before

- 메시지 도메인은 **REST API만** 제공 (POST/GET /messages)
- 클라이언트가 새 메시지를 받으려면 **폴링** 또는 **수동 새로고침** 필요
- 실시간 알림·수신 경험 부족, 메시지 서비스 핵심 가치인 **즉시 전달** 미달성
- WebSocket/SSE 등 실시간 채널에 대한 **기술 비교·선택·재연결·성능 측정** 계획 없음

## Goal

1. **실시간 메시지 수신**  
   - 발송 즉시 수신자에게 푸시 (폴링 제거)
2. **기술 선택 근거 마련**  
   - WebSocket vs SSE vs Long Polling 비교 후, 서비스 요구사항에 맞는 방식 선정
3. **연결 안정성**  
   - 연결 관리, 재연결 전략 설계 및 적용
4. **성능 정량화**  
   - latency, throughput 측정으로 목표치 설정 및 모니터링 기반 마련

## Key Decision

**WebSocket (STOMP) 방식 채택**

### 선택 이유

1. **안정성**: 300개 동시 연결까지 **에러율 0%**, 모든 메시지 100% 수신
2. **성능 일관성**: 연결 수 증가해도 레이턴시 안정적 (P50: 91~257ms)
3. **메시지 보장**: STOMP ACK 메커니즘으로 메시지 전달 보장 가능
4. **양방향 통신**: 향후 클라이언트→서버 메시지 필요 시 확장 용이
5. **검증된 기술**: 대규모 서비스에서 널리 사용, 레퍼런스 풍부

### SSE 대비 장점

- **메시지 손실 0%**: SSE는 1~4% 메시지 미수신, WebSocket은 0%
- **양방향 통신**: SSE는 단방향만 가능
- **명확한 ACK**: STOMP 프로토콜로 메시지 수신 확인 가능

### 트레이드오프

- **구현 복잡도**: SSE 대비 설정 복잡 (WebSocketConfig, STOMP, Interceptor)
- **프록시 설정**: Nginx `Upgrade` 헤더 처리 필요
- **개발 시간**: SSE 대비 초기 구축 시간 증가

→ **대규모 서비스 안정성 > 초기 구현 복잡도** 판단

## Impact

### 긍정적 영향

1. **사용자 경험 개선**
   - 폴링 제거 → 실시간 메시지 수신 (레이턴시 ~100ms)
   - 메시지 유실 없음 → 신뢰성 확보

2. **서버 리소스 최적화**
   - 폴링 제거 → 불필요한 HTTP 요청 90% 이상 감소
   - Long-lived 연결로 연결 오버헤드 최소화

3. **확장 가능성**
   - 양방향 통신으로 향후 기능 확장 용이 (타이핑 표시, 읽음 확인 등)
   - Redis Pub/Sub 연동으로 멀티 서버 구조 대응 가능

### 주의사항

1. **운영 복잡도 증가**
   - Nginx 프록시 설정 추가 (`Upgrade`, `Connection` 헤더)
   - Sticky Session 필요 (수평 확장 시)
   - 연결 상태 모니터링 필요

2. **인프라 의존성**
   - Redis Pub/Sub 필수 (멀티 서버 환경)
   - 서버 재시작 시 모든 클라이언트 재연결 필요

---

## 진행 목표

1. WebSocket 구현 (POC: 연결, 구독/수신 — 송신은 POST /messages 유지)
2. SSE 구현 (POC)
3. Long Polling 구현 (POC)
4. 세 방식 비교
   - 4-1. 리소스: 연결당 CPU/메모리, 동시 연결 한계
   - 4-2. 성능: latency, throughput
   - 4-3. 운영·개발: 재연결, LB, 코드 복잡도
5. (비교 결과 반영) 1개 선정 → 프로덕션용 구현


## 진행

### 1. WebSocket 구현

- **의존성**: `spring-boot-starter-websocket` (STOMP 메시징용)
- **엔드포인트**: `/ws`, SockJS fallback 허용
- **인증**: Handshake 시 `Authorization` 또는 `access_token` 쿼리에서 JWT 검증 → `Principal`에 `userId` 넣기 (기존 `AuthTokenResolver` 재사용)
- **구독(수신)**: `/topic/chat-rooms/{chatRoomId}/messages` — 채팅방별 메시지 스트림 구독
- **송신**: WebSocket 제외. 기존 **POST /messages** 사용. 단, 생성 후 `SimpMessagingTemplate.convertAndSend(/topic/chat-rooms/{id}/messages, message)` 로 구독자에게 브로드캐스트 (MessageController 또는 ApplicationService에서)
- **POC 범위**: 연결·구독(수신)만 (재연결, Heartbeat 등은 4번 비교 이후)

#### 구현 관점 장단점

**장점**
- 양방향 통신 가능 (클라이언트→서버, 서버→클라이언트)
- STOMP 프로토콜로 메시지 라우팅, 구독 패턴이 명확
- SockJS fallback으로 브라우저 호환성 확보
- Redis Pub/Sub 등 메시지 브로커로 서버 간 메시지 동기화 구조가 명확

**단점**
- 설정 복잡도 높음 (WebSocketConfig, MessageBroker, Interceptor 등)
- HTTP 표준이 아니므로 프록시/LB 설정 추가 필요 (Upgrade 헤더 처리)
- STOMP 의존성 추가로 번들 사이즈 증가
- 연결 관리 복잡 (Handshake, Session, Subscription 분리 관리)
- Sticky Session 필수 (수평 확장 시 연결이 특정 서버에 고정)
- 서버 재시작 시 모든 클라이언트 재연결 필요
- Redis 등 추가 인프라 필수 (서버 간 메시지 동기화)

### 2. SSE 구현 ✅

- **엔드포인트**: `GET /sse/chat-rooms/{chatRoomId}/messages`
- **인증**: 기존 `AuthTokenResolver` 재사용 (`Authorization` 헤더)
- **구독(수신)**: 채팅방별 메시지 스트림 구독 (타임아웃: 30분)
- **송신**: WebSocket과 동일. 기존 **POST /messages** 사용, `MessageEvent.Created` 이벤트로 SSE 브로드캐스트
- **구현**:
  - `MessageSseEmitterManager`: 채팅방별 SseEmitter 관리 (추가/제거/전송)
  - `MessageSseController`: SSE 엔드포인트 제공
  - `MessageCreatedSseEventListener`: `MessageEvent.Created` → SSE 브로드캐스트
  - `MessageSseIT`: SSE 통합 테스트
- **POC 범위**: 연결·구독(수신)만 (재연결, Heartbeat 등은 4번 비교 이후)

#### 구현 관점 장단점

**장점**
- 구현 단순 (`SseEmitter` 하나면 완결, 추가 의존성 불필요)
- HTTP 표준 (`text/event-stream`), 프록시/LB 설정 불필요
- 기존 인증 필터 그대로 재사용 (Spring MVC 레이어에서 처리)
- 자동 재연결 지원 (브라우저 EventSource API 내장)
- HTTP 기반이라 일반 LB 전략 사용 가능
- 브라우저 자동 재연결로 서버 장애 시 다른 서버로 자동 전환

**단점**
- 단방향 통신만 가능 (서버→클라이언트)
- 브라우저 연결 수 제한 (HTTP/1.1: 도메인당 6개, HTTP/2는 무제한)
- 타임아웃 관리 필요 (장시간 연결 시 주기적 heartbeat 권장)
- IE 미지원 (polyfill 필요)
- Sticky Session 필요 (수평 확장 시)
- Redis Pub/Sub 등 메시지 브로커 필수 (서버 간 메시지 동기화)

### 3. Long Polling 구현 ✅

- **엔드포인트**: `GET /long-polling/chat-rooms/{chatRoomId}/messages`
- **인증**: 기존 `AuthTokenResolver` 재사용 (`Authorization` 헤더)
- **수신**: 새 메시지가 있을 때까지 대기 (타임아웃: 30초)
  - 새 메시지 생성 시 즉시 응답
  - 타임아웃 시 빈 리스트 `[]` 응답
- **송신**: 기존 **POST /messages** 사용, `MessageEvent.Created` 이벤트로 대기 중인 요청에 응답
- **구현**:
  - `MessageLongPollingManager`: 채팅방별 DeferredResult 관리 (추가/제거/응답)
  - `MessageLongPollingController`: Long Polling 엔드포인트 제공
  - `MessageCreatedLongPollingEventListener`: `MessageEvent.Created` → Long Polling 응답
  - `MessageLongPollingIT`: Long Polling 통합 테스트 (메시지 수신, 타임아웃)
- **POC 범위**: 연결·대기(수신)만 (재연결 전략, Since 파라미터 등은 4번 비교 이후)

#### 구현 관점 장단점

**장점**
- 구현 단순 (`DeferredResult` 하나면 완결, 추가 의존성 불필요)
- HTTP 표준 (일반 GET 요청), 프록시/LB 설정 불필요
- 기존 인증 필터 그대로 재사용 (Spring MVC 레이어에서 처리)
- 브라우저 호환성 최대 (IE 포함 모든 브라우저 지원)
- HTTP 기반이라 일반 LB 전략 사용 가능

**단점**
- 연결 효율 낮음 (요청-응답-재요청 반복, HTTP 오버헤드)
- 지연 시간 높음 (클라이언트가 재요청할 때까지 지연, 평균 RTT/2 추가)
- 서버 리소스 비효율 (대기 중인 스레드 또는 비동기 리소스 점유)
- 타임아웃 관리 복잡 (짧으면 재요청 빈번, 길면 리소스 점유 증가)
- Sticky Session 필요 (수평 확장 시)
- Redis Pub/Sub 등 메시지 브로커 필수 (서버 간 메시지 동기화)

### 4. 세 방식 비교

가장 먼저, 구현 전 알고 있는 내용을 바탕으로 1차 정리해본 뒤

아래 지표를 통해 성능을 측정해볼 예정입니다.
1. Latency 측정
2. 동시 연결 부하 테스트
3. 장시간 연결 안정성 테스트

#### 4-1. 알고있는 내용을 바탕으로 1차 정리

| 항목 | WebSocket | SSE | Long Polling |
|------|-----------|-----|--------------|
| 구현 난이도 | 높음 (STOMP 설정) | 낮음 (SseEmitter) | 낮음 (DeferredResult) |
| 통신 방향 | 양방향 | 단방향 | 단방향 |
| 브라우저 지원 | 넓음 (SockJS fallback) | 중간 (IE 미지원) | 최대 (모든 브라우저) |
| 재연결 | 수동 구현 필요 | 자동 (EventSource) | 수동 구현 필요 |
| 재연결 빈도 | 낮음 (네트워크 끊김 시) | 중간 (타임아웃마다) | 높음 (매 메시지) |
| 메시지 누락 방지 | 수동 (ACK/메시지ID) | 표준 제공 (Last-Event-ID) | 수동 (since 파라미터) |
| 프록시/LB | 설정 필요 | 불필요 | 불필요 |
| 예상 Latency | 낮음 (~10ms) | 낮음 (~10ms) | 중간 (~50ms) |
| 리소스 효율 | 높음 | 높음 | 낮음 |
| 운영 복잡도 | 높음 | 낮음 | 낮음 |

→ **플랫폼별 최적 선택**:
- **웹 서비스**: SSE 유리 (브라우저 EventSource 표준, 구현 단순, 자동 재연결, Last-Event-ID 지원)
- **앱 서비스**: WebSocket 유리 (양방향 통신, 배터리 효율, Keep-Alive 제어, 네이티브 지원 우수)

#### 4-2. 연속 메시지 수신 지연 (Burst Latency) 테스트 ✅

**테스트**: 10명 구독자, 메시지 3개 연속 전송(50ms 간격), 200회 반복, 네트워크 지연 50ms

**테스트 코드**: `04-test/02-latency-message-realtime/`
- `test-latency-burst.js`: 메인 테스트 로직
- `run-test-burst.ps1`: 실행 스크립트
- `client-*.js`: WebSocket/SSE/Long Polling 클라이언트 구현

**결과** (단위: ms):

| Method | 성공/실패 | P50 | P95 | P99 | Max |
|--------|-----------|-----|-----|-----|-----|
| **WebSocket** | 200/0 | 346 | 375 | 381 | 381 |
| **SSE** | 200/0 | 355 | 377 | 380 | 380 |
| **Long Polling** | 198/2 | 356 | **32,735** | **32,741** | **32,741** |

**분석**:
- WebSocket/SSE: 연결 유지 → 모든 메시지 즉시 수신 (P99 ~380ms)
- Long Polling 문제:
  - 재연결 중 발송된 메시지는 **수신 불가** (메시지 유실)
  - 유실된 경우 **타임아웃(30초)까지 대기** → P95+ 폭증
  - P50: 356ms (정상), P95: 32,735ms (타임아웃)

**결론**: 
- **WebSocket/SSE**: 안정적이고 예측 가능 → 최종 후보
- **Long Polling**: 5%는 30초+ 지연, 메시지 유실 가능 → **제외** (Fallback으로만 고려)

#### 4-3. 동시 연결 부하 테스트 ✅

**테스트**: 50~300개 동시 연결, 단계별 메시지 전송 후 latency 측정, 네트워크 지연 50ms

**테스트 코드**: `04-test/02-latency-message-realtime/`
- `test-concurrent-load.js`: 메인 부하 테스트 로직
- `run-test-load.ps1`: 실행 스크립트
- `client-*.js`: WebSocket/SSE 클라이언트 구현

**결과** (단위: ms):

**WebSocket**:
| Connections | Success | Fail | Received | ErrorRate(%) | P50 | P95 | P99 |
|------------|---------|------|----------|--------------|-----|-----|-----|
| 50 | 50 | 0 | 50 | 0.00 | 152 | 153 | 153 |
| 100 | 100 | 0 | 100 | 0.00 | 93 | 100 | 101 |
| 150 | 150 | 0 | 150 | 0.00 | 91 | 100 | 101 |
| 200 | 200 | 0 | 200 | 0.00 | 135 | 142 | 143 |
| 250 | 250 | 0 | 250 | 0.00 | 257 | 262 | 263 |
| 300 | 300 | 0 | 300 | 0.00 | 203 | 210 | 211 |

**SSE** (Nginx worker_connections 증가 전, 1024):
| Connections | Success | Fail | Received | ErrorRate(%) | P50 | P95 | P99 | 비고 |
|------------|---------|------|----------|--------------|-----|-----|-----|------|
| 50 | 49 | 1 | 49 | 2.00 | 94 | 99 | 100 | 1건 타임아웃 |
| 100 | 100 | 0 | 97 | 3.00 | 101 | 113 | 116 | 3건 미수신 |
| 150 | 150 | 0 | 148 | 1.33 | 103 | 120 | 122 | 2건 미수신 |
| 200 | 200 | 0 | 198 | 1.00 | 323 | 349 | 352 | 2건 미수신 |
| 250 | 54 | 196 | 51 | **79.60** | 222 | 230 | 230 | **196건 502 에러** |
| 300 | 105 | 195 | 102 | **66.00** | 203 | 217 | 217 | **195건 502 에러** |

**SSE** (Nginx worker_connections 증가 후, 10000):
| Connections | Success | Fail | Received | ErrorRate(%) | P50 | P95 | P99 | 비고 |
|------------|---------|------|----------|--------------|-----|-----|-----|------|
| 50 | 50 | 0 | 48 | 4.00 | 92 | 99 | 100 | 2건 미수신 |
| 100 | 100 | 0 | 98 | 2.00 | 380 | 417 | 417 | 2건 미수신 |
| 150 | 150 | 0 | 148 | 1.33 | 581 | 602 | 604 | 2건 미수신 |
| 200 | 200 | 0 | 198 | 1.00 | 363 | 390 | 393 | 2건 미수신 |
| 250 | 250 | 0 | 247 | **1.20** | 215 | 239 | 242 | 3건 미수신 |
| 300 | 299 | 1 | 297 | **1.00** | 240 | 283 | 287 | 1건 타임아웃, 2건 미수신 |

**분석**:
- **WebSocket**: 
  - 300개 연결까지 **안정적** (에러율 0%)
  - P50 레이턴시: 87~426ms (네트워크 지연 50ms 포함)
  - 연결 수 증가해도 **성능 일정** 유지
  
- **SSE**:
  - **Nginx worker_connections 증가 전**: 250개부터 **급격한 성능 저하** (79.6% 에러율, HTTP 502)
  - **Nginx worker_connections 증가 후**: 300개까지 **안정적** (에러율 1~4%, 메시지 미수신만)
  - **원인 분석**:
    
    **1. HTTP 502 에러 (250개 이상, 해결됨 ✅)**:
    - **Nginx worker_connections 제한** (기본값: 1024개)
    - SSE 연결 1개 = **Nginx 연결 2개** (클라이언트↔Nginx, Nginx↔백엔드)
    - 250개 SSE 연결 = 500개 Nginx 연결 + 기타 HTTP 요청 → **1024개 초과**
    - Nginx가 새 연결을 받지 못해 → "no live upstreams" → **502 에러**
    - **해결**: `worker_connections 10000`으로 증가 → 502 에러 **완전히 제거**
    - WebSocket도 동일하게 연결 2개 필요하지만, NIO 기반으로 더 효율적
    
    **2. 메시지 미수신 (전 구간, 1~4%)**:
    - SSE 전송 실패 시 해당 Emitter 자동 제거 (복구 불가)
    - 네트워크 불안정, 클라이언트 타임아웃 등으로 일부 연결 끊김
    - EventSource 자동 재연결 중 메시지 발송 시 유실
    - WebSocket은 STOMP ACK로 메시지 전달 보장 가능
  
**결론**: 
- **WebSocket**: 높은 동시 연결 수에서도 **안정적** (에러율 0%) → 최종 후보 1순위
- **SSE**: Nginx 설정 후 300개까지 **안정적** (에러율 1~4%, 미수신만)
  - 프록시 설정 주의: `worker_connections` 충분히 확보 필요
  - 메시지 미수신 1~4%는 네트워크 특성상 불가피 (Last-Event-ID로 복구 가능)
- 대규모 동시 접속 + 메시지 보장이 중요한 경우 **WebSocket 선택 권장**

### 최종 선택: WebSocket ✅

위 테스트 결과를 바탕으로 **WebSocket (STOMP)** 방식을 최종 선택합니다.

**핵심 근거**:

1. **완벽한 안정성**: 50~300개 모든 구간에서 **에러율 0%**, 메시지 손실 0건 (SSE는 1~4% 손실)
2. **메시지 보장**: STOMP ACK로 프로토콜 수준 전달 보장 (SSE는 수동 복구 필요)
3. **일관된 성능**: 연결 수 증가해도 레이턴시 안정 (P50: 91~257ms), SSE는 92ms→581ms로 2배+ 증가
4. **비즈니스 임팩트**: SSE의 1~4% 에러는 일 10만 메시지 시 **1,000~4,000건 유실** → 사용자 신뢰 하락
5. **SLA 달성**: 예측 가능한 성능으로 장애 대응 용이, 운영 목표 수립 가능
6. **양방향 통신**: 타이핑 표시, 읽음 확인 등 추가 기능 구현 가능 (SSE는 단방향만)
7. **프로토콜 표준화**: STOMP로 Web/iOS/Android 클라이언트 다양화 대응 용이
8. **인프라 독립성**: SSE는 Nginx `worker_connections` 오류 시 502 폭증, WebSocket은 독립적
9. **검증된 기술**: 대규모 서비스 레퍼런스 풍부 → 트러블슈팅 빠름, 기술 부채 최소화
10. **장기적 이득**: 초기 설정 복잡도 감수 → 안정적 운영으로 유지보수 비용 절감

**다음 단계** (우선순위별):

### P0: 프로덕션 배포 전 필수
1. WebSocket 프로덕션 구현 (재연결, Heartbeat, 에러 핸들링)
2. 보안 강화 (WSS, CORS, Rate Limiting: Token Bucket/Leaky Bucket)
3. 프로토콜 문서화 (엔드포인트, 메시지 포맷, 에러 코드, 클라이언트 연동 가이드)
4. 기본 모니터링 구축 (연결 상태, 메시지 전송 성공률, Grafana/Prometheus)

### P1: 초기 운영 안정화
5. 로깅 및 추적 (Structured Logging, Distributed Tracing: OpenTelemetry/Jaeger/Zipkin)
6. SLO/SLI 정의 (Latency SLI: P99<500ms, Availability: 99.9%, Message Delivery: 99.99%, Error Budget)

### P2: 스케일 대비
7. Redis Pub/Sub 연동 (멀티 서버 환경 대응)
8. 어플리케이션 수평 확장 방안 (Sticky Session, 로드 밸런싱)
9. 대규모 부하 테스트 (1,000~10,000개 연결, CPU/메모리/네트워크 병목 파악)
10. 장애 시나리오 테스트 (Chaos Engineering: 서버 재시작/Redis 장애/네트워크 파티션, Graceful Degradation, Circuit Breaker)

### P3: 미래 확장
11. MQTT 확장 대비 (IoT/모바일 앱 확장 시 MQTT 브로커 도입, WebSocket 병행 운영)

---

## 추가 고려: MQTT

### 개요

MQTT(Message Queuing Telemetry Transport)는 IoT 환경을 위해 설계된 경량 Pub/Sub 메시징 프로토콜입니다. WebSocket/SSE와 함께 실시간 통신 방식으로 고려할 수 있습니다.

### 기본 정보

- **프로토콜**: MQTT 3.1.1 / 5.0 (바이너리 프로토콜)
- **브로커**: Mosquitto, HiveMQ, EMQX, EMQ X, VerneMQ 등
- **포트**: TCP 1883 (일반), 8883 (TLS), 9001 (WebSocket)
- **전송 방식**: Pub/Sub 모델 (토픽 기반 메시지 라우팅)

### 구현 방식

**서버 측**:
- MQTT 브로커 인프라 구축 (Mosquitto/HiveMQ 등)
- Spring Integration MQTT 또는 Eclipse Paho 클라이언트
- `MessageEvent.Created` → MQTT Publish (`chat-rooms/{chatRoomId}/messages`)

**클라이언트 측**:
- **웹**: MQTT.js (WebSocket 기반 MQTT-over-WS)
- **앱**: Eclipse Paho (Android/iOS), Swift-MQTT

**인증**:
- MQTT Username/Password
- TLS 클라이언트 인증서
- JWT 토큰 (브로커 플러그인/확장 필요)

### 장단점

**장점**:
- **경량 프로토콜**: 헤더 2바이트, HTTP 대비 대역폭 효율 매우 높음
- **양방향 통신**: Pub/Sub 모델로 클라이언트↔서버 양방향 통신 가능
- **QoS 지원**: 메시지 전달 보장 수준 선택 가능
  - QoS 0: At most once (최대 1회, 전송만)
  - QoS 1: At least once (최소 1회, 중복 가능)
  - QoS 2: Exactly once (정확히 1회, 4-way handshake)
- **불안정한 네트워크에 강함**: Keep-alive, 자동 재연결 내장
- **배터리 효율**: IoT/모바일 환경 최적화 (최소한의 패킷 교환)
- **Last Will & Testament**: 클라이언트 비정상 종료 시 자동 알림
- **Retained Message**: 신규 구독자에게 마지막 메시지 자동 전달
- **Topic 필터링**: 와일드카드 구독 (`chat-rooms/+/messages`)

**단점**:
- **브로커 인프라 필수**: Mosquitto/HiveMQ 등 추가 운영 필요
- **웹 브라우저 지원 제한**: WebSocket 기반 MQTT-over-WS만 가능 (일반 TCP MQTT 불가)
- **프록시/LB 설정 복잡**: TCP 기반이므로 HTTP 기반 LB 사용 불가, TCP LB 필요
- **인증/인가 복잡**: 기존 JWT 인증 통합 어려움, 브로커별 플러그인 개발 필요
- **디버깅 어려움**: 바이너리 프로토콜, 브라우저 DevTools 사용 불가
- **학습 곡선**: Pub/Sub 모델, QoS, Topic 설계 등 개념 이해 필요
- **운영 복잡도 증가**: 브로커 모니터링, 스케일링, 장애 대응 등 추가 부담
- **오버킬 가능성**: 단순 채팅 서비스에는 과도한 기능 (브로커 운영 부담 > 이점)

### WebSocket/SSE와 비교

| 항목 | WebSocket | SSE | MQTT |
|------|-----------|-----|------|
| 구현 난이도 | 높음 (STOMP 설정) | 낮음 (SseEmitter) | 높음 (브로커 인프라) |
| 통신 방향 | 양방향 | 단방향 | 양방향 |
| 브라우저 지원 | 넓음 (SockJS fallback) | 중간 (IE 미지원) | 제한적 (WS 기반 필요) |
| 재연결 | 수동 구현 필요 | 자동 (EventSource) | 자동 (내장) |
| 메시지 보장 | 수동 (ACK/메시지ID) | 표준 (Last-Event-ID) | QoS 지원 (0/1/2) |
| 프록시/LB | 설정 필요 | 불필요 | 설정 매우 복잡 (TCP) |
| 예상 Latency | 낮음 (~10ms) | 낮음 (~10ms) | 매우 낮음 (~5ms) |
| 대역폭 효율 | 중간 | 중간 | 높음 (헤더 2B) |
| 리소스 효율 | 높음 | 높음 | 매우 높음 |
| 운영 복잡도 | 높음 | 낮음 | 매우 높음 |
| 추가 인프라 | Redis Pub/Sub | Redis Pub/Sub | MQTT 브로커 필수 |

### 적합한 사용 사례

**MQTT가 유리한 경우**:
- IoT 디바이스 연동 (센서, 스마트홈 등)
- 모바일 앱 중심 (배터리 효율 중요)
- 불안정한 네트워크 환경 (3G/LTE 불안정)
- 초대규모 동시 접속 (수십만 연결)
- 메시지 전달 보장이 매우 중요 (QoS 2)

**WebSocket/SSE가 유리한 경우**:
- 웹 브라우저 중심 서비스
- HTTP 인프라 재사용 (기존 LB, 프록시)
- 간단한 인증/인가 (JWT)
- 빠른 개발 및 배포
- 소규모~중규모 동시 접속 (~수천 연결)

### 결론

현재 프로젝트는 **웹 기반 채팅 서비스**이므로:
- **1순위**: WebSocket (안정성, 메시지 보장, 브라우저 지원)
- **2순위**: SSE (구현 단순성, HTTP 표준)
- **고려 대상**: MQTT (IoT/모바일 확장 시 검토)

**MQTT는 오버킬 가능성**:
- 브로커 운영 부담 > 성능 이점
- 웹 브라우저 지원 제한 (WebSocket 기반 필요 → WebSocket 직접 사용이 더 단순)
- 기존 HTTP 인프라 재사용 불가

**추후 고려 시나리오**:
- IoT 디바이스 연동 필요 (스마트홈, 센서 등)
- 모바일 앱 네이티브 개발 (배터리 효율 중요)
- 초대규모 확장 (수십만 동시 접속)
